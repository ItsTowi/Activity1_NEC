import numpy as np
import warnings
from sklearn.preprocessing import StandardScaler

class NeuralNet:
    def __init__(self, layers, epochs, lr, momentum, function, perc_validation):
        self.L = len(layers)    #Number of layers
        self.n = layers.copy()  #An array with the number of units in each layer

        self.h = []             #An array of arrays for the fields (h)
        for lay in range(self.L):
            self.h.append(np.zeros(layers[lay]))
        self.xi = []            #An array of arrays for the activations(Xi)
        for lay in range(self.L):
            self.xi.append(np.zeros(layers[lay]))

        self.w = []             #An array of matrices for the weights
        self.w.append(np.zeros((1, 1)))
        for lay in range(1, self.L):
            self.w.append(np.zeros((layers[lay], layers[lay - 1])))

        self.theta = []         #An array of arrays for thresholds
        for lay in range(self.L):
            self.theta.append(np.zeros(layers[lay]))
        
        self.delta = []         #An array of arrays for the propagation errors
        for lay in range(self.L):
            self.delta.append(np.zeros(layers[lay]))
            
        self.d_w = []           #An array of matrices for the changes on weights
        self.d_w.append(np.zeros((1, 1)))
        for lay in range(1, self.L):
            self.d_w.append(np.zeros((layers[lay], layers[lay - 1])))
        
        self.d_theta = []       #An array of arrays for the changes on thresholds
        for lay in range(self.L):
            self.d_theta.append(np.zeros(layers[lay]))
            
        self.d_w_prev = []      #An array of matrices for the previoius changes of the 
                                #weights used for the momentum term
        self.d_w_prev.append(np.zeros((1, 1)))
        for lay in range(1, self.L):
            self.d_w_prev.append(np.zeros((layers[lay], layers[lay - 1])))
            
        self.d_theta_prev = []  #An array of arrays for the previous changes of the 
                                #thresholds used for the momentum term
        for lay in range(self.L):
            self.d_theta_prev.append(np.zeros(layers[lay]))                     
        
        self.fact = function      #The name of the activation function that will be 
                                #used (sigmoid, relu, linear, tanh)
        
        self.epochs = epochs
        self.lr = lr
        self.momentum = momentum
        self.perc_validation = perc_validation
        
        self.train_errors = []
        self.val_errors = []
                                
    def _activation(self, x):
        if self.fact == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif self.fact == 'tanh':
            return np.tanh(x)
        elif self.fact == 'relu':
            return np.maximum(0, x)
        elif self.fact == 'linear':
            return x
        else:
            raise ValueError(f"Function {self.fact} not supported")

    def _activation_derivative(self, x):
        if self.fact == 'sigmoid':
            s = self._activation(x)
            return s * (1 - s)
        elif self.fact == 'tanh':
            s = self._activation(x)
            return 1 - s**2
        elif self.fact == 'relu':
            return (x > 0).astype(float)
        elif self.fact == 'linear':
            return np.ones_like(x)
        else:
            raise ValueError(f"Función {self.fact} no soportada")

    def initialize_w_theta(self):
        for L in range(1, self.L):
            n_in = self.n[L - 1]  
            n_out = self.n[L]    
            
            # Fórmula de Xavier/Glorot para inicialización uniforme
            limit = np.sqrt(6.0 / (n_in + n_out))
            
            # Inicialización de Pesos (w)
            self.w[L] = np.random.uniform(low=-limit, high=limit, size=(n_out, n_in))
            
            # Inicialización de Umbrales (theta) a cero (práctica común y segura)
            self.theta[L] = np.zeros(n_out)
    
    def feed_forward(self, X):
        self.xi[0] = X
        for L in range(1, self.L):
            # h[L] = w[L] * xi[L-1] - theta[L]
            self.h[L] = self.w[L].dot(self.xi[L-1]) - self.theta[L]

            # xi[L] = f(h[L])
            self.xi[L] = self._activation(self.h[L])

        return self.xi[self.L - 1]
        
    def activation_function(self, i, L):
        self.h[L][i] = 0
        for j in range(0, self.n[L-1]):
            self.h[L][i] += self.w[L][i, j] * self.xi[L-1][j]
            
        self.h[L][i] -= self.theta[L][i]
         
        return self._activation(self.h[L][i])
        #return (1 / (1 + np.exp(-self.h[L][i])))
    
    def error_back_propagation(self, out_x, y):

        L = self.L - 1
        # Capa de salida (vectorizado, correcto)
        self.delta[L] = (out_x - y) * self._activation_derivative(self.h[L])
        # Capas ocultas (también vectorizado)
        for l in range(self.L - 2, 0, -1):
            self.delta[l] = self._activation_derivative(self.h[l]) * (self.w[l+1].T.dot(self.delta[l+1]))
                 
    def update_values(self):
        for l in range(1, self.L):
            for i in range(0, self.n[l]):
                self.d_theta[l][i]= (self.lr * self.delta[l][i]) + (self.momentum * self.d_theta_prev[l][i])
                self.theta[l][i] += self.d_theta[l][i]
                for j in range(0, self.n[l - 1]):
                    self.d_w[l][i,j] = -self.lr * self.delta[l][i] * self.xi[l-1][j] + self.momentum * self.d_w_prev[l][i,j]
                    self.w[l][i,j] += self.d_w[l][i,j]

        self.d_w_prev = [dw.copy() for dw in self.d_w]
        self.d_theta_prev = [dt.copy() for dt in self.d_theta]

    def shuffle_data(self, X, y):
        if hasattr(X, 'values'):
            X = X.values
        if hasattr(y, 'values'):
            y = y.values
        if y.ndim == 1:
            y = y.reshape(-1, 1)
            
        indices = np.random.permutation(X.shape[0])
        return X[indices], y[indices]

    def data_division(self, X, y):
        if y.ndim == 1:
            y = y.reshape(-1, 1)

        n_total = X.shape[0]
        n_test = int(0.20 * n_total)

        X_test = X[-n_test:]
        y_test = y[-n_test:]

        X_train_val = X[:-n_test]
        y_train_val = y[:-n_test]
        
        n_train_val = X_train_val.shape[0]
        n_val = int(self.perc_validation * n_train_val)

        X_val = X_train_val[:n_val]
        y_val = y_train_val[:n_val]

        X_train = X_train_val[n_val:]
        y_train = y_train_val[n_val:]
        
        self.X_train, self.y_train = X_train, y_train
        self.X_val,   self.y_val   = X_val,   y_val
        self.X_test,  self.y_test  = X_test,  y_test

        print("Shuffle y splits realizados:")
        print(f"  Train: {X_train.shape[0]} patrones")
        print(f"  Val:   {X_val.shape[0]} patrones")
        print(f"  Test:  {X_test.shape[0]} patrones")
    
    def calculate_quadratic_error(self, X_set, y_set):
        total_error = 0.0
        num_patterns = X_set.shape[0]

        for mu in range(num_patterns):
            x_mu = X_set[mu]
            z_mu = y_set[mu]

            # 1. Feed-forward: Obtener la predicción o(x^mu)
            o_mu = self.feed_forward(x_mu)

            squared_difference = (o_mu - z_mu)**2
            pattern_error_sum = np.sum(squared_difference) 
            
            total_error += pattern_error_sum

        return 0.5 * total_error
    
    def fit(self,X,y):
        
        self.initialize_w_theta()
        X, y = self.shuffle_data(X, y)
        self.data_division(X, y)
        num_train = self.X_train.shape[0]
        val_err = np.nan
        
        for epoch in range(self.epochs):
            for _ in range(num_train):
                idx = np.random.randint(0, num_train)   # patrón aleatorio
                #Choose a random pattern (xu zu) of training set
                output = self.feed_forward(self.X_train[idx])
                #print(f"Output {output}")
                
                #Back-propagate the error for this pattern
                self.error_back_propagation(output, self.y_train[idx])
                
                #Update the weights and threseholds
                self.update_values()
            
            train_err = self.calculate_quadratic_error(self.X_train, self.y_train)
            self.train_errors.append(train_err)
            
            if self.perc_validation > 0:
                val_err = self.calculate_quadratic_error(self.X_val, self.y_val)
                self.val_errors.append(val_err)
            
            print(f"Epoch {epoch}: Train Error={train_err:.4f}, Val Error={val_err:.4f}")
        
    def predict(self, X):
        if not isinstance(X, np.ndarray):
            X = np.array(X, dtype=float)

        all_scaled_predictions = []
        X_input = X

        for idx, x in enumerate(X_input):
            
            scaled_prediction = self.feed_forward(x) 
            all_scaled_predictions.append(scaled_prediction[0])
            
        scaled_predictions_array = np.array(all_scaled_predictions).reshape(-1, 1)
        
        if hasattr(self, 'y_scaler') and isinstance(self.y_scaler, StandardScaler):
            
            final_predictions = self.y_scaler.inverse_transform(scaled_predictions_array)
            
            return final_predictions
        
        return scaled_predictions_array
    
    def loss_epochs(self):
        return np.array(self.train_errors), np.array(self.val_errors)
